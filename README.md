# ğŸ¬ GFLIX Data Processing â€” Movies & Ratings

Projeto de processamento local de arquivos CSV (movies + customers_rating), geraÃ§Ã£o de arquivo **Parquet** e upload para o **Amazon S3**.

O pipeline foi desenvolvido em Python e pode ser executado localmente por qualquer pessoa apenas configurando variÃ¡veis de ambiente.

---

## âœ… Funcionalidades

* Leitura de CSV de entrada (`movies.csv` e `customers_rating.csv`)
* Tratamento e limpeza das tabelas
* NormalizaÃ§Ã£o dos nomes das colunas
* ConversÃ£o das datas e tipos
* JunÃ§Ã£o entre as tabelas via `movie_id`
* ExportaÃ§Ã£o local em formato **Parquet**
* Upload automÃ¡tico para o S3 usando `boto3`

---

## ğŸ“ Estrutura do Projeto

```
ğŸ“¦ gflix-data-processing
 â”£ ğŸ“„ app.py
 â”£ ğŸ“„ requirements.txt
 â”— ğŸ“„ README.md
```

---

## âš™ï¸ PrÃ©-requisitos

* Python 3.9+
* pip
* Conta AWS
* S3 Bucket criado (ex: `work-final-five-gflix`)
* Access Key & Secret Key vÃ¡lidas (para uso local)

---

## ğŸ”§ InstalaÃ§Ã£o

1. Clone o repositÃ³rio:

```bash
git clone https://github.com/gmln-user/aws-final-project
cd aws-final-project
```

2. Crie um ambiente virtual:

```bash
python -m venv .venv
```

3. Ative o ambiente:

* Windows:

```bash
.\.venv\Scripts\activate
```

* Linux/Mac:

```bash
source .venv/bin/activate
```

4. Instale as dependÃªncias:

```bash
pip install -r requirements.txt
```

---

## ğŸ” ConfiguraÃ§Ã£o do `.env`

Crie um arquivo chamado **.env** na raiz do projeto e configure as variÃ¡veis:

```
AWS_ACCESS_KEY=SEU_ACCESS_KEY
AWS_SECRET_KEY=SEU_SECRET_KEY
AWS_REGION=us-east-1

BUCKET_NAME=work-final-five-gflix
OUTPUT_KEY=processed/final_data.parquet

MOVIES_PATH=movies.csv
CUSTOMERS_PATH=customers_rating.csv
```

> VocÃª pode usar o arquivo `.env.example` como base.

---

## â–¶ï¸ ExecuÃ§Ã£o do Pipeline

ApÃ³s configurar as variÃ¡veis:

```bash
python app.py
```

O fluxo serÃ¡:

1. Processamento dos arquivos CSV
2. Join entre customers Ã— movies
3. GeraÃ§Ã£o do arquivo local:

```
final_data.parquet
```

4. Upload automÃ¡tico ao S3:

```
s3://work-final-five-gflix/processed/final_data.parquet
```

---

## ğŸ§ª Teste RÃ¡pido

ApÃ³s rodar:

```bash
aws s3 ls s3://work-final-five-gflix/processed/
```

VocÃª deverÃ¡ ver:

```
2025-11-10  22:01:12   45283423 final_data.parquet
```

---

## âš ï¸ ObservaÃ§Ãµes Importantes

* NÃ£o compartilhe credenciais do AWS.
* Para ambientes de produÃ§Ã£o, use IAM roles (sem access keys).
* Para arquivos muito grandes, recomenda-se o uso de AWS Lambda + S3 streaming ou AWS Glue.

---

## ğŸ§¹ Limpeza

Para sair do ambiente virtual:

```bash
deactivate
```

---

## ğŸ“œ LicenÃ§a

MIT License.

---

Se quiser, eu posso melhorar o README com badges, imagem do workflow ou diagramas da arquitetura.


